# ğŸš€ Apache Spark

Apache Spark is an **open-source**, **distributed computing system** designed to process **large-scale datasets** quickly and efficiently.

### ğŸ”§ Key Advantages
1. âš¡ **Very Fast Processing** â€“ Utilizes in-memory computation.
2. ğŸŒ **General Purpose** â€“ Compatible with any storage or resource.
3. â±ï¸ **Versatile** â€“ Supports both batch and real-time processing.

---

## âœ¨ CHARACTERISTICS

### 1. ğŸ§  In-Memory Processing
- Spark processes data **in memory**, unlike traditional disk-based systems like MapReduce.
- **MapReduce (left)**: 5 Spark jobs = 10 disk R/W operations  
- **Spark (right)**: 5 Spark jobs = ~2 memory R/E operations  
![MapReduce VS Spark](images/image-1.png)

### 2. ğŸ› ï¸ Ease of Use
- Spark offers APIs for major languages: **Java**, **Python**, **R**, and **Scala**.
- Writing Spark applications is intuitive and developer-friendly.

### 3. ğŸ§© Unified Framework
- Spark supports diverse workloads:
  - Batch Processing
  - Real-time Streaming
  - Machine Learning
  - Graph Processing

---

## ğŸŒŸ FEATURES

- âš¡ **Fast Execution**
- ğŸ“ˆ **Highly Scalable**
- ğŸ›¡ï¸ **Fault Tolerance** via DAG (Directed Acyclic Graph)
- ğŸ—£ï¸ **Polyglot Support**
- ğŸ”„ **Unified Performance**
- ğŸ”Œ **Pluggable Architecture** â€“ Loosely coupled components

<br>

## SPARK ECOSYSTEM

- **Spark Core engine** handles all the functionalities like:
    - Scheduling Jobs.
    - Input / Output Operations.
    - Monitoring Jobs.

![SPARK ECOSYSTEM Diagram](images/image.png)


## EXECUTION PLAN

**HDFS file &rarr; Read in Spark (as RDD) &rarr; Processing &rarr; Save it.**

![Execution Plan](images/image-2.png)

## Transformations & Actions

![D/W Txn & Actions](images/image-3.png)

- The **transformations are LAZY** because it applies the operations on **WHAT IS NEEDED** and not the whole data as a whole:

![Lazy transformations visulation](images/image-4.png)


## JOB, TASK, STAGE 

![Job, Task, Stage](images/image-7.png)

- A **Job** is divided into **Stages** based on wide transformations, and each **Stage** runs multiple **Tasks** depending on the number of partitions.

### ğŸ” Spark Job Breakdown Example

Letâ€™s say you run a Spark job to process a dataset with **4 partitions** and apply **two wide transformations** (`groupBy`, `join`):

- **Job**: 1 job triggered by an action like `collect()` or `save()`.
- **Stages**: 2 wide transformations + 1 = **3 stages**.
- **Tasks**: Each stage will run **4 tasks** (one per partition).

---

### ğŸ“Š Execution Flow

| Job         | Stage 1              | Stage 2              | Stage 3              |
|-------------|----------------------|----------------------|----------------------|
| `collect()` | `groupBy` â†’ 4 tasks  | `join` â†’ 4 tasks     | Final action â†’ 4 tasks |

> ğŸ’¡ Each stage executes tasks in parallel across partitions, enabling distributed and scalable processing.


## DATAFRAME
![Dataframe intro](images/image-8.png)

- This can be used to create dataframe &rarr; `df = spark.createDataFrame(data, columns)`

- `df.printSchema` - To see the schema of the data.

## DATATYPES IN PYSPARK 
![Datatypes in pyspark](images/image-9.png)

## CACHING IN SPARK
![Caching](images/image-10.png)